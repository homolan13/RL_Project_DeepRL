{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Project: Bolus Glucose Control in Type 1 Diabetes Using Deep Reinforcement Learning\n",
    "Raphael Joost, 18-???-??? & Yanis SchÃ¤rer, 18-114-058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Deep Deterministic Policy Gradient (DDPG) agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, device, max_size=int(1e6)):\n",
    "        self.device = device\n",
    "        self.max_size = max_size\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "        self.state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.action_buffer = np.zeros((max_size, action_dim))\n",
    "        self.next_state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.reward_buffer = np.zeros((max_size, 1))\n",
    "        self.done_buffer = np.zeros((max_size, 1))\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.state_buffer[self.ptr] = state\n",
    "        self.action_buffer[self.ptr] = action\n",
    "        self.next_state_buffer[self.ptr] = next_state\n",
    "        self.reward_buffer[self.ptr] = reward\n",
    "        self.done_buffer[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            th.tensor(self.state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.action_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.reward_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.next_state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.done_buffer[idx], dtype=th.float32).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor and Critic networks\n",
    "class Actor(th.nn.Module): # state -> action\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dim, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * th.tanh(self.l3(a))\n",
    "\n",
    "class Critic(th.nn.Module): # state + action -> Q(s,a) (Q-Network)\n",
    "    def __init__(self, state_dimension, action_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dimension + action_dimension, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = F.relu(self.l1(th.cat([state, action], 1)))\n",
    "        q = F.relu(self.l2(q))\n",
    "        return self.l3(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agents\n",
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, device, discount=0.99, tau=0.005):\n",
    "        self.device = device\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        # Actor and Actor target\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_optimizer = th.optim.Adam(self.actor.parameters())\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # Critic and Critic target\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_optimizer = th.optim.Adam(self.critic.parameters())\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "    def select_action(self, state): # Actor selects action based on current state\n",
    "        state = th.FloatTensor(state).reshape(1, -1).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(local_model, target_model, tau): # Soft update of target parameters\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        # Sample from replay buffer\n",
    "        state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n",
    "        # Compute the target Q value\n",
    "        target_q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_q = reward + (done * self.discount * target_q).detach()\n",
    "        # Get current Q estimate\n",
    "        current_q = self.critic(state, action)\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        # Update the target models\n",
    "        DDPGAgent.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        DDPGAgent.soft_update(self.actor, self.actor_target, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the DDPG agent to the OpenAI Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='simglucose-adolescent2-v0',\n",
    "    entry_point='simglucose.envs:T1DSimEnv',\n",
    "    kwargs={'patient_name': 'adolescent#002'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('simglucose-adolescent2-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "action_dimension = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "agent = DDPGAgent(state_dimension, action_dimension, max_action, device)\n",
    "memory = ReplayBuffer(state_dimension, action_dimension, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Score: -23.582171751194014\n",
      "Episode: 1 Score: -4.754356228833474\n",
      "Episode: 2 Score: -16.158880647234902\n",
      "Episode: 3 Score: -5.047603681832058\n",
      "Episode: 4 Score: -0.19435636738973416\n",
      "Episode: 5 Score: -4.253971629207639\n",
      "Episode: 6 Score: -2.540037593980623\n",
      "Episode: 7 Score: 0.1718842987123208\n",
      "Episode: 8 Score: -9.80442246600418\n",
      "Episode: 9 Score: -8.175262554804752\n",
      "Episode: 10 Score: -6.113026963773256\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m      8\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mselect_action(state)\n\u001b[1;32m----> 9\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     10\u001b[0m     memory\u001b[39m.\u001b[39mstore(state, action, next_state, reward, done)\n\u001b[0;32m     11\u001b[0m     state \u001b[39m=\u001b[39m next_state\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\core.py:96\u001b[0m, in \u001b[0;36mEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     81\u001b[0m     \u001b[39m\"\"\"Run one timestep of the environment's dynamics. When end of\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[39m    episode is reached, you are responsible for calling `reset()`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    to reset this environment's state.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39m        info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step(action)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\simglucose\\envs\\simglucose_gym_env.py:45\u001b[0m, in \u001b[0;36mT1DSimEnv._step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m act \u001b[39m=\u001b[39m Action(basal\u001b[39m=\u001b[39maction, bolus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fun \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(act)\n\u001b[0;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(act, reward_fun\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fun)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\simglucose\\simulation\\env.py:77\u001b[0m, in \u001b[0;36mT1DSimEnv.step\u001b[1;34m(self, action, reward_fun)\u001b[0m\n\u001b[0;32m     73\u001b[0m CGM \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_time)):\n\u001b[0;32m     76\u001b[0m     \u001b[39m# Compute moving average as the sample measurements\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     tmp_CHO, tmp_insulin, tmp_BG, tmp_CGM \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmini_step(action)\n\u001b[0;32m     78\u001b[0m     CHO \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tmp_CHO \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_time\n\u001b[0;32m     79\u001b[0m     insulin \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tmp_insulin \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_time\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\simglucose\\simulation\\env.py:58\u001b[0m, in \u001b[0;36mT1DSimEnv.mini_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     55\u001b[0m patient_mdl_act \u001b[39m=\u001b[39m Action(insulin\u001b[39m=\u001b[39minsulin, CHO\u001b[39m=\u001b[39mCHO)\n\u001b[0;32m     57\u001b[0m \u001b[39m# State update\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpatient\u001b[39m.\u001b[39;49mstep(patient_mdl_act)\n\u001b[0;32m     60\u001b[0m \u001b[39m# next observation\u001b[39;00m\n\u001b[0;32m     61\u001b[0m BG \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatient\u001b[39m.\u001b[39mobservation\u001b[39m.\u001b[39mGsub\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\simglucose\\patient\\t1dpatient.py:116\u001b[0m, in \u001b[0;36mT1DPatient.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_odesolver\u001b[39m.\u001b[39mset_f_params(action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_Qsto,\n\u001b[0;32m    114\u001b[0m                              \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_foodtaken)\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_odesolver\u001b[39m.\u001b[39msuccessful():\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_odesolver\u001b[39m.\u001b[39;49mintegrate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_odesolver\u001b[39m.\u001b[39;49mt \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_time)\n\u001b[0;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m'\u001b[39m\u001b[39mODE solver failed!!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\scipy\\integrate\\_ode.py:431\u001b[0m, in \u001b[0;36mode.integrate\u001b[1;34m(self, t, step, relax)\u001b[0m\n\u001b[0;32m    428\u001b[0m     mth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_integrator\u001b[39m.\u001b[39mrun\n\u001b[0;32m    430\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 431\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m=\u001b[39m mth(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjac \u001b[39mor\u001b[39;49;00m (\u001b[39mlambda\u001b[39;49;00m: \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    432\u001b[0m                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt, t,\n\u001b[0;32m    433\u001b[0m                           \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf_params, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjac_params)\n\u001b[0;32m    434\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSystemError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    435\u001b[0m     \u001b[39m# f2py issue with tuple returns, see ticket 1187.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    437\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mFunction to integrate must not return a tuple.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    438\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\scipy\\integrate\\_ode.py:1174\u001b[0m, in \u001b[0;36mdopri5.run\u001b[1;34m(self, f, jac, y0, t0, t1, f_params, jac_params)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, f, jac, y0, t0, t1, f_params, jac_params):\n\u001b[1;32m-> 1174\u001b[0m     x, y, iwork, istate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunner(\u001b[39m*\u001b[39;49m((f, t0, y0, t1) \u001b[39m+\u001b[39;49m\n\u001b[0;32m   1175\u001b[0m                                       \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_args) \u001b[39m+\u001b[39;49m (f_params,)))\n\u001b[0;32m   1176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mistate \u001b[39m=\u001b[39m istate\n\u001b[0;32m   1177\u001b[0m     \u001b[39mif\u001b[39;00m istate \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\simglucose\\patient\\t1dpatient.py:182\u001b[0m, in \u001b[0;36mT1DPatient.model\u001b[1;34m(t, x, action, params, last_Qsto, last_foodtaken)\u001b[0m\n\u001b[0;32m    179\u001b[0m dxdt[\u001b[39m5\u001b[39m] \u001b[39m=\u001b[39m (x[\u001b[39m5\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m dxdt[\u001b[39m5\u001b[39m]\n\u001b[0;32m    181\u001b[0m \u001b[39m# insulin action on glucose utilization\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m dxdt[\u001b[39m6\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mparams\u001b[39m.\u001b[39mp2u \u001b[39m*\u001b[39m x[\u001b[39m6\u001b[39m] \u001b[39m+\u001b[39m params\u001b[39m.\u001b[39;49mp2u \u001b[39m*\u001b[39m (It \u001b[39m-\u001b[39m params\u001b[39m.\u001b[39mIb)\n\u001b[0;32m    184\u001b[0m \u001b[39m# insulin action on production\u001b[39;00m\n\u001b[0;32m    185\u001b[0m dxdt[\u001b[39m7\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mparams\u001b[39m.\u001b[39mki \u001b[39m*\u001b[39m (x[\u001b[39m7\u001b[39m] \u001b[39m-\u001b[39m It)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\pandas\\core\\generic.py:5574\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5566\u001b[0m \u001b[39m# Note: obj.x will always call obj.__getattribute__('x') prior to\u001b[39;00m\n\u001b[0;32m   5567\u001b[0m \u001b[39m# calling obj.__getattr__('x').\u001b[39;00m\n\u001b[0;32m   5568\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5569\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5570\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5571\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5572\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5573\u001b[0m ):\n\u001b[1;32m-> 5574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[name]\n\u001b[0;32m   5575\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\pandas\\core\\series.py:944\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m--> 944\u001b[0m     check_deprecated_indexers(key)\n\u001b[0;32m    945\u001b[0m     key \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m)\n\u001b[0;32m    947\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39mis\u001b[39;00m \u001b[39mEllipsis\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\pandas\\core\\indexing.py:2486\u001b[0m, in \u001b[0;36mcheck_deprecated_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2474\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2475\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m   2476\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[0;32m   2477\u001b[0m \u001b[39m    bool\u001b[39;00m\n\u001b[0;32m   2478\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2479\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m   2480\u001b[0m         obj\u001b[39m.\u001b[39mstart \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2481\u001b[0m         \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39mstop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2482\u001b[0m         \u001b[39mor\u001b[39;00m (obj\u001b[39m.\u001b[39mstep \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m obj\u001b[39m.\u001b[39mstep \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m   2483\u001b[0m     )\n\u001b[1;32m-> 2486\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_deprecated_indexers\u001b[39m(key) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2487\u001b[0m     \u001b[39m\"\"\"Checks if the key is a deprecated indexer.\"\"\"\u001b[39;00m\n\u001b[0;32m   2488\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   2489\u001b[0m         \u001b[39misinstance\u001b[39m(key, \u001b[39mset\u001b[39m)\n\u001b[0;32m   2490\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(key, \u001b[39mtuple\u001b[39m)\n\u001b[0;32m   2491\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mset\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m   2492\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score_history = []\n",
    "num_episodes = 1000\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        memory.store(state, action, next_state, reward, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "    score_history.append(score)\n",
    "    agent.train(memory)\n",
    "    print(f'Episode: {episode} Score: {score}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "362460f2204f0406e663750cba4e7c84597df365884d41ed13c51949e389925d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
