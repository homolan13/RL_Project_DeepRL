{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Project: Bolus Glucose Control in Type 1 Diabetes Using Deep Reinforcement Learning\n",
    "Raphael Joost, 18-???-??? & Yanis SchÃ¤rer, 18-114-058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Deep Deterministic Policy Gradient (DDPG) agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, device, max_size=int(1e6)):\n",
    "        self.device = device\n",
    "        self.max_size = max_size\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "        self.state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.action_buffer = np.zeros((max_size, action_dim))\n",
    "        self.next_state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.reward_buffer = np.zeros((max_size, 1))\n",
    "        self.done_buffer = np.zeros((max_size, 1))\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.state_buffer[self.ptr] = state\n",
    "        self.action_buffer[self.ptr] = action\n",
    "        self.next_state_buffer[self.ptr] = next_state\n",
    "        self.reward_buffer[self.ptr] = reward\n",
    "        self.done_buffer[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            th.tensor(self.state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.action_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.reward_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.next_state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.done_buffer[idx], dtype=th.float32).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor and Critic networks\n",
    "class Actor(th.nn.Module): # state -> action\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dim, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * th.tanh(self.l3(a))\n",
    "\n",
    "class Critic(th.nn.Module): # state + action -> Q(s,a) (Q-Network)\n",
    "    def __init__(self, state_dimension, action_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dimension + action_dimension, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = F.relu(self.l1(th.cat([state, action], 1)))\n",
    "        q = F.relu(self.l2(q))\n",
    "        return self.l3(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agents\n",
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, device, discount=0.99, tau=0.005):\n",
    "        self.device = device\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        # Actor and Actor target\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_optimizer = th.optim.Adam(self.actor.parameters())\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # Critic and Critic target\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_optimizer = th.optim.Adam(self.critic.parameters())\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "    def select_action(self, state): # Actor selects action based on current state\n",
    "        state = th.FloatTensor(state.reshape(1, -1)).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(local_model, target_model, tau): # Soft update of target parameters\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        # Sample from replay buffer\n",
    "        state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n",
    "        # Compute the target Q value\n",
    "        target_q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_q = reward + (done * self.discount * target_q).detach()\n",
    "        # Get current Q estimate\n",
    "        current_q = self.critic(state, action)\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        # Update the target models\n",
    "        DDPGAgent.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        DDPGAgent.soft_update(self.actor, self.actor_target, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the DDPG agent to the OpenAI Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='simglucose-adolescent2-v0',\n",
    "    entry_point='simglucose.envs:T1DSimEnv',\n",
    "    kwargs={'patient_name': 'adolescent#002'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yanis\\anaconda3\\envs\\rl\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('simglucose-adolescent2-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "action_dimension = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "agent = DDPGAgent(state_dimension, action_dimension, max_action, device)\n",
    "memory = ReplayBuffer(state_dimension, action_dimension, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "362460f2204f0406e663750cba4e7c84597df365884d41ed13c51949e389925d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
