{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Project: Bolus Glucose Control in Type 1 Diabetes Using Deep Reinforcement Learning\n",
    "Raphael Joost, 18-???-??? & Yanis SchÃ¤rer, 18-114-058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Deep Deterministic Policy Gradient (DDPG) agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanis\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "import simglucose\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, device, max_size=int(1e6)):\n",
    "        self.device = device\n",
    "        self.max_size = max_size\n",
    "        self.size = 0\n",
    "        self.ptr = 0\n",
    "        self.state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.action_buffer = np.zeros((max_size, action_dim))\n",
    "        self.next_state_buffer = np.zeros((max_size, state_dim))\n",
    "        self.reward_buffer = np.zeros((max_size, 1))\n",
    "        self.done_buffer = np.zeros((max_size, 1))\n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        self.state_buffer[self.ptr] = state\n",
    "        self.action_buffer[self.ptr] = action\n",
    "        self.next_state_buffer[self.ptr] = next_state\n",
    "        self.reward_buffer[self.ptr] = reward\n",
    "        self.done_buffer[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            th.tensor(self.state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.action_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.reward_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.next_state_buffer[idx], dtype=th.float32).to(self.device),\n",
    "            th.tensor(self.done_buffer[idx], dtype=th.float32).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Actor and Critic networks\n",
    "class Actor(th.nn.Module): # state -> action\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dim, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        return self.max_action * th.tanh(self.l3(a))\n",
    "\n",
    "class Critic(th.nn.Module): # state + action -> Q(s,a) (Q-Network)\n",
    "    def __init__(self, state_dimension, action_dimension):\n",
    "        super(Critic, self).__init__()\n",
    "        self.l1 = th.nn.Linear(state_dimension + action_dimension, 400)\n",
    "        self.l2 = th.nn.Linear(400, 300)\n",
    "        self.l3 = th.nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q = F.relu(self.l1(th.cat([state, action], 1)))\n",
    "        q = F.relu(self.l2(q))\n",
    "        return self.l3(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Agents\n",
    "class DDPGAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, device, discount=0.99, tau=0.005):\n",
    "        self.device = device\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        # Actor and Actor target\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_optimizer = th.optim.Adam(self.actor.parameters())\n",
    "        self.actor_target = deepcopy(self.actor)\n",
    "        # Critic and Critic target\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_optimizer = th.optim.Adam(self.critic.parameters())\n",
    "        self.critic_target = deepcopy(self.critic)\n",
    "        \n",
    "    def select_action(self, state): # Actor selects action based on current state\n",
    "        state = th.FloatTensor(state).reshape(1, -1).to(self.device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    @staticmethod\n",
    "    def soft_update(local_model, target_model, tau): # Soft update of target parameters\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=100):\n",
    "        # Sample from replay buffer\n",
    "        state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n",
    "        # Compute the target Q value\n",
    "        target_q = self.critic_target(next_state, self.actor_target(next_state))\n",
    "        target_q = reward + (done * self.discount * target_q).detach()\n",
    "        # Get current Q estimate\n",
    "        current_q = self.critic(state, action)\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        # Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        # Optimize the actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        # Update the target models\n",
    "        DDPGAgent.soft_update(self.critic, self.critic_target, self.tau)\n",
    "        DDPGAgent.soft_update(self.actor, self.actor_target, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward function based on paper\n",
    "def paper_reward(BG_history):\n",
    "    BG = BG_history[-1]\n",
    "    # BG: blood glucose level\n",
    "    # Hypoglycemia: BG < 70 mg/dL\n",
    "    if 30 <= BG and BG < 70:\n",
    "        return -1.5\n",
    "    # Normoglycemia: 70 mg/dL < BG < 180 mg/dL\n",
    "    elif 70 <= BG and BG <= 180:\n",
    "        return 0.5\n",
    "    # Hyperglycemia: BG > 180 mg/dL\n",
    "    elif 180 < BG and BG <= 300:\n",
    "        return -0.8\n",
    "    elif 300 < BG and BG <= 350:\n",
    "        return -1\n",
    "    # Other cases\n",
    "    else:\n",
    "        return -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linking the DDPG agent to the OpenAI Gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simglucose.simulation.scenario import CustomScenario\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "scenarios = []\n",
    "\n",
    "for i in range(100):\n",
    "    min = rng.integers(0,60)\n",
    "    h = rng.integers(0,24)\n",
    "    start = datetime(2023, 1, 4, h, min, 0)\n",
    "    scenarios.append(CustomScenario(start_time=start, scenario=[])) # scenario: list of tuples (offset from start, meal intake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='simglucose-bolus',\n",
    "    entry_point='simglucose.envs:T1DSimEnv',\n",
    "    kwargs={'patient_name': ['adolescent#001', 'adolescent#002', 'adolescent#003',\n",
    "        'adolescent#004', 'adolescent#005', 'adolescent#006', 'adolescent#007',\n",
    "        'adolescent#008', 'adolescent#009', 'adolescent#010'],\n",
    "        'history_length': 12, 'reward_fun': paper_reward, 'custom_scenario': scenarios,\n",
    "        'enable_meal': True, 'enable_bolus': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('simglucose-bolus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenObservation(gym.ActionWrapper):     \n",
    "    \"\"\"Action wrapper that flattens the action.\"\"\"\n",
    "    def init(self, env):\n",
    "        super(FlattenObservation, self).init(env)         \n",
    "        self.observation_space = gym.spaces.utils.flatten_space(self.env.observation_space)              \n",
    "    # def action(self, action):         \n",
    "    #     return gym.spaces.utils.unflatten(self.env.action_space, action)      \n",
    "    # def reverse_action(self, action):        \n",
    "    #     return gym.spaces.utils.flatten(self.env.action_space, action)\n",
    "\n",
    "class FlattenAction(gym.ActionWrapper):     \n",
    "    \"\"\"Action wrapper that flattens the action.\"\"\"\n",
    "    def init(self, env):\n",
    "        super(FlattenAction, self).init(env)         \n",
    "        self.action_space = gym.spaces.utils.flatten_space(self.env.action_space)              \n",
    "    def action(self, action):         \n",
    "        return gym.spaces.utils.unflatten(self.env.action_space, action)      \n",
    "    def reverse_action(self, action):        \n",
    "        return gym.spaces.utils.flatten(self.env.action_space, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlattenObservation(env)\n",
    "env = FlattenAction(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict('CGM': Box(0.0, 10000.0, (12,), float32), 'CHO': Box(0.0, 10000.0, (12,), float32), 'insulin': Box(0.0, 10000.0, (12,), float32))\n",
      "Dict('basal': Box(0.0, 30.0, (), float32), 'bolus': Box(0.0, 30.0, (), float32))\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m th\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m state_dimension \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mobservation_space\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m      4\u001b[0m action_dimension \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m      5\u001b[0m max_action \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n",
    "\n",
    "state_dimension = env.observation_space.shape[0]\n",
    "action_dimension = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "agent = DDPGAgent(state_dimension, action_dimension, max_action, device)\n",
    "memory = ReplayBuffer(state_dimension, action_dimension, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('RL_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad990b79a96f46e3047ccc5edfd4b21cd82c9588551cb1a4cd9c5d3e22519b4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
